{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk Phase: A More Robust Baseline with LightGBM\n",
    "\n",
    "Our simple baseline showed that the hidden test set is very different from the training data. This means we need a model that can generalize better.\n",
    "\n",
    "**This notebook improves on the baseline in three key ways:**\n",
    "1.  **Text Cleaning:** We will add a function to convert text to lowercase and remove noise like links and punctuation. This helps the model focus on meaningful words.\n",
    "2.  **Better Features (N-grams):** We will configure our `TfidfVectorizer` to see two-word phrases (`ngram_range=(1, 2)`), which provides more context than single words alone.\n",
    "3.  **A More Powerful Model (LightGBM):** We will replace `LogisticRegression` with `LightGBM`, a gradient boosting model that is a standard in Kaggle competitions for its ability to learn complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Feature Engineering and Text Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the text cleaning function\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning text data...\n",
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# Create the initial combined_text feature\n",
    "df['combined_text'] = df['rule'] + \" [SEP] \" + df['body']\n",
    "test_df['combined_text'] = test_df['rule'] + \" [SEP] \" + test_df['body']\n",
    "\n",
    "# Apply the cleaning function\n",
    "print(\"Cleaning text data...\")\n",
    "df['cleaned_text'] = df['combined_text'].apply(clean_text)\n",
    "test_df['cleaned_text'] = test_df['combined_text'].apply(clean_text)\n",
    "print(\"Cleaning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Training and Validation\n",
    "First, we'll get a new local validation score with this stronger pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features (X) and target (y)\n",
    "X = df['cleaned_text']\n",
    "y = df['rule_violation']\n",
    "\n",
    "# Create a training and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the improved TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),      # Use 1-word and 2-word phrases\n",
    "    max_features=15000,      # Limit vocabulary size to the top 15k features\n",
    "    stop_words='english'     # Remove common English stop words\n",
    ")\n",
    "\n",
    "# Fit and transform the training data, then transform the validation data\n",
    "X_train_vec = vectorizer.fit_transform(X_train)\n",
    "X_val_vec = vectorizer.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LightGBM model for validation...\n",
      "[LightGBM] [Info] Number of positive: 825, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001571 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 8906\n",
      "[LightGBM] [Info] Number of data points in the train set: 1623, number of used features: 184\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508318 -> initscore=0.033275\n",
      "[LightGBM] [Info] Start training from score 0.033275\n",
      "New Validation AUC Score with LightGBM: 0.7763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admn\\work_area\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the LightGBM model\n",
    "print(\"Training LightGBM model for validation...\")\n",
    "lgbm = lgb.LGBMClassifier(objective='binary', random_state=42)\n",
    "lgbm.fit(X_train_vec, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "val_preds = lgbm.predict_proba(X_val_vec)[:, 1]\n",
    "auc_score = roc_auc_score(y_val, val_preds)\n",
    "\n",
    "print(f\"New Validation AUC Score with LightGBM: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Final Submission Pipeline\n",
    "Now we'll use this improved pipeline to train on all the data and generate a new submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define final data\n",
    "X_full = df['cleaned_text']\n",
    "y_full = df['rule_violation']\n",
    "X_test = test_df['cleaned_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Initialize a NEW vectorizer and model for the final submission\n",
    "final_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=15000,\n",
    "    stop_words='english'\n",
    ")\n",
    "final_model = lgb.LGBMClassifier(objective='binary', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training final LightGBM model on the full dataset...\n",
      "[LightGBM] [Info] Number of positive: 1031, number of negative: 998\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 9727\n",
      "[LightGBM] [Info] Number of data points in the train set: 2029, number of used features: 246\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508132 -> initscore=0.032531\n",
      "[LightGBM] [Info] Start training from score 0.032531\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Fit vectorizer and train model on ALL training data\n",
    "print(\"Training final LightGBM model on the full dataset...\")\n",
    "X_full_vec = final_vectorizer.fit_transform(X_full)\n",
    "final_model.fit(X_full_vec, y_full)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admn\\work_area\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Make predictions on the test set\n",
    "X_test_vec = final_vectorizer.transform(X_test)\n",
    "test_predictions = final_model.predict_proba(X_test_vec)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUCCESS: New submission_lgbm.csv has been generated.\n",
      "Here are the first 5 predictions:\n",
      "   row_id  rule_violation\n",
      "0    2029        0.341977\n",
      "1    2030        0.669740\n",
      "2    2031        0.851339\n",
      "3    2032        0.520972\n",
      "4    2033        0.931842\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Create and save the submission file\n",
    "submission_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'rule_violation': test_predictions\n",
    "})\n",
    "submission_df.to_csv('submission_lgbm.csv', index=False)\n",
    "\n",
    "print(\"\\nSUCCESS: New submission_lgbm.csv has been generated.\")\n",
    "print(\"Here are the first 5 predictions:\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
