{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "917ae587",
   "metadata": {},
   "source": [
    "# Run Phase 2: Upgrading the Embedding Model to MPNet\n",
    "\n",
    "Our previous experiments showed:\n",
    "1. Similarity features are the correct strategy (Score: 0.705).\n",
    "2. Adding more engineered features from those similarities did not help (Score: 0.696).\n",
    "\n",
    "**New Hypothesis:** The current features are good, but their quality can be improved by using a more powerful sentence embedding model. We will upgrade from `all-MiniLM-L6-v2` to the state-of-the-art `all-mpnet-base-v2`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393bdf99",
   "metadata": {},
   "source": [
    "### 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87396ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Joao Gabriel\\OneDrive\\work_area\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import lightgbm as lgb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13fc76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418ce512",
   "metadata": {},
   "source": [
    "### 2. Generate Embeddings with the Upgraded MPNet Model\n",
    "**Note:** This step will be slower than with MiniLM as the model is larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa39606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SentenceTransformer model from: ./all-mpnet-base-v2-local/\n",
      "Model loaded successfully.\n",
      "Generating embeddings for: body\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 64/64 [01:32<00:00,  1.44s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for: positive_example_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 64/64 [01:22<00:00,  1.29s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for: positive_example_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 64/64 [01:19<00:00,  1.24s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for: negative_example_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 64/64 [01:09<00:00,  1.09s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings for: negative_example_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 64/64 [01:07<00:00,  1.06s/it]\n",
      "Batches: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it]\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Add your new Kaggle dataset and update this path!\n",
    "model_path = './all-mpnet-base-v2-local/'\n",
    "\n",
    "print(f\"Loading SentenceTransformer model from: {model_path}\")\n",
    "embed_model = SentenceTransformer(model_path)\n",
    "print(\"Model loaded successfully.\")\n",
    "\n",
    "text_cols = ['body', 'positive_example_1', 'positive_example_2', 'negative_example_1', 'negative_example_2']\n",
    "\n",
    "for col in text_cols:\n",
    "    print(f\"Generating embeddings for: {col}\")\n",
    "    df[f'{col}_vec'] = embed_model.encode(df[col].astype(str).tolist(), show_progress_bar=True).tolist()\n",
    "    test_df[f'{col}_vec'] = embed_model.encode(test_df[col].astype(str).tolist(), show_progress_bar=True).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a8b8bf",
   "metadata": {},
   "source": [
    "### 3. Create Similarity Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c4382ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating similarity features...\n",
      "Similarity features created.\n"
     ]
    }
   ],
   "source": [
    "def calculate_similarity(df_row, vec_col_1, vec_col_2):\n",
    "    vec1 = np.array(df_row[vec_col_1]).reshape(1, -1)\n",
    "    vec2 = np.array(df_row[vec_col_2]).reshape(1, -1)\n",
    "    return cosine_similarity(vec1, vec2)[0][0]\n",
    "\n",
    "print(\"Calculating similarity features...\")\n",
    "for df_ in [df, test_df]:\n",
    "    df_['sim_pos_1'] = df_.apply(lambda row: calculate_similarity(row, 'body_vec', 'positive_example_1_vec'), axis=1)\n",
    "    df_['sim_pos_2'] = df_.apply(lambda row: calculate_similarity(row, 'body_vec', 'positive_example_2_vec'), axis=1)\n",
    "    df_['sim_neg_1'] = df_.apply(lambda row: calculate_similarity(row, 'body_vec', 'negative_example_1_vec'), axis=1)\n",
    "    df_['sim_neg_2'] = df_.apply(lambda row: calculate_similarity(row, 'body_vec', 'negative_example_2_vec'), axis=1)\n",
    "print(\"Similarity features created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472af651",
   "metadata": {},
   "source": [
    "### 4. Train Model on the Higher-Quality Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bb2b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== FOLD 1 =====\n",
      "[LightGBM] [Info] Number of positive: 825, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001522 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 1623, number of used features: 4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508318 -> initscore=0.033275\n",
      "[LightGBM] [Info] Start training from score 0.033275\n",
      "===== FOLD 2 =====\n",
      "[LightGBM] [Info] Number of positive: 825, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000503 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 1623, number of used features: 4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508318 -> initscore=0.033275\n",
      "[LightGBM] [Info] Start training from score 0.033275\n",
      "===== FOLD 3 =====\n",
      "[LightGBM] [Info] Number of positive: 825, number of negative: 798\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000653 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 1623, number of used features: 4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508318 -> initscore=0.033275\n",
      "[LightGBM] [Info] Start training from score 0.033275\n",
      "===== FOLD 4 =====\n",
      "[LightGBM] [Info] Number of positive: 824, number of negative: 799\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000660 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 1623, number of used features: 4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.507702 -> initscore=0.030810\n",
      "[LightGBM] [Info] Start training from score 0.030810\n",
      "===== FOLD 5 =====\n",
      "[LightGBM] [Info] Number of positive: 825, number of negative: 799\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000492 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1020\n",
      "[LightGBM] [Info] Number of data points in the train set: 1624, number of used features: 4\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.508005 -> initscore=0.032022\n",
      "[LightGBM] [Info] Start training from score 0.032022\n",
      "\n",
      "Overall CV AUC Score with MPNet: 0.6140\n"
     ]
    }
   ],
   "source": [
    "# We are reverting to the original 4 features, as we proved they work best\n",
    "features = ['sim_pos_1', 'sim_pos_2', 'sim_neg_1', 'sim_neg_2']\n",
    "X = df[features]\n",
    "y = df['rule_violation']\n",
    "X_test = test_df[features]\n",
    "\n",
    "NFOLDS = 5\n",
    "skf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = np.zeros((len(df),))\n",
    "test_preds = np.zeros((len(test_df),))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"===== FOLD {fold+1} =====\")\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = lgb.LGBMClassifier(objective='binary', random_state=42, n_estimators=500)\n",
    "    model.fit(X_train, y_train, \n",
    "              eval_set=[(X_val, y_val)], \n",
    "              eval_metric='auc', \n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    \n",
    "    val_fold_preds = model.predict_proba(X_val)[:, 1]\n",
    "    test_fold_preds = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    oof_preds[val_idx] = val_fold_preds\n",
    "    test_preds += test_fold_preds / NFOLDS\n",
    "\n",
    "overall_cv_score = roc_auc_score(y, oof_preds)\n",
    "print(f\"\\nOverall CV AUC Score with MPNet: {overall_cv_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a95fdd",
   "metadata": {},
   "source": [
    "### 5. Create Final Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0c22fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: New submission_mpnet.csv has been generated.\n",
      "   row_id  rule_violation\n",
      "0    2029        0.499974\n",
      "1    2030        0.425424\n",
      "2    2031        0.518751\n",
      "3    2032        0.452337\n",
      "4    2033        0.626429\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.DataFrame({\n",
    "    'row_id': test_df['row_id'],\n",
    "    'rule_violation': test_preds\n",
    "})\n",
    "submission_df.to_csv('submission_mpnet.csv', index=False)\n",
    "\n",
    "print(\"SUCCESS: New submission_mpnet.csv has been generated.\")\n",
    "print(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
